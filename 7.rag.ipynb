{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始调用 RunnableSequence {\n",
      "  lc_serializable: true,\n",
      "  lc_kwargs: {\n",
      "    first: RunnableMap {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: {\n",
      "        steps: { context: [RunnableSequence], question: [RunnableLambda] }\n",
      "      },\n",
      "      lc_runnable: true,\n",
      "      name: undefined,\n",
      "      lc_namespace: [ \"langchain_core\", \"runnables\" ],\n",
      "      steps: {\n",
      "        context: RunnableSequence {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_runnable: true,\n",
      "          name: undefined,\n",
      "          first: [RunnableLambda],\n",
      "          middle: [Array],\n",
      "          last: [RunnableLambda],\n",
      "          lc_namespace: [Array]\n",
      "        },\n",
      "        question: RunnableLambda {\n",
      "          lc_serializable: false,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_runnable: true,\n",
      "          name: undefined,\n",
      "          lc_namespace: [Array],\n",
      "          func: [Function: question]\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    middle: [\n",
      "      ChatPromptTemplate {\n",
      "        lc_serializable: true,\n",
      "        lc_kwargs: {\n",
      "          inputVariables: [Array],\n",
      "          promptMessages: [Array],\n",
      "          partialVariables: [Object: null prototype] {}\n",
      "        },\n",
      "        lc_runnable: true,\n",
      "        name: undefined,\n",
      "        lc_namespace: [ \"langchain_core\", \"prompts\", \"chat\" ],\n",
      "        inputVariables: [ \"context\", \"question\" ],\n",
      "        outputParser: undefined,\n",
      "        partialVariables: [Object: null prototype] {},\n",
      "        promptMessages: [ [HumanMessagePromptTemplate] ],\n",
      "        validateTemplate: true\n",
      "      }\n",
      "    ],\n",
      "    last: Ollama {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: {\n",
      "        callbacks: undefined,\n",
      "        baseUrl: \"http://localhost:11434\",\n",
      "        model: \"llama3.1\"\n",
      "      },\n",
      "      lc_runnable: true,\n",
      "      name: undefined,\n",
      "      verbose: false,\n",
      "      callbacks: undefined,\n",
      "      tags: [],\n",
      "      metadata: {},\n",
      "      caller: AsyncCaller {\n",
      "        maxConcurrency: Infinity,\n",
      "        maxRetries: 6,\n",
      "        onFailedAttempt: [Function: defaultFailedAttemptHandler],\n",
      "        queue: PQueue {\n",
      "          _events: Events <Complex prototype> {},\n",
      "          _eventsCount: 0,\n",
      "          _intervalCount: 0,\n",
      "          _intervalEnd: 0,\n",
      "          _pendingCount: 0,\n",
      "          _resolveEmpty: [Function: empty],\n",
      "          _resolveIdle: [Function: empty],\n",
      "          _carryoverConcurrencyCount: false,\n",
      "          _isIntervalIgnored: true,\n",
      "          _intervalCap: Infinity,\n",
      "          _interval: 0,\n",
      "          _queue: [PriorityQueue],\n",
      "          _queueClass: [class PriorityQueue],\n",
      "          _concurrency: Infinity,\n",
      "          _intervalId: undefined,\n",
      "          _timeout: undefined,\n",
      "          _throwOnTimeout: false,\n",
      "          _isPaused: false\n",
      "        }\n",
      "      },\n",
      "      cache: undefined,\n",
      "      _encoding: undefined,\n",
      "      lc_namespace: [ \"langchain\", \"llms\", \"ollama\" ],\n",
      "      model: \"llama3.1\",\n",
      "      baseUrl: \"http://localhost:11434\",\n",
      "      keepAlive: \"5m\",\n",
      "      embeddingOnly: undefined,\n",
      "      f16KV: undefined,\n",
      "      frequencyPenalty: undefined,\n",
      "      logitsAll: undefined,\n",
      "      lowVram: undefined,\n",
      "      mainGpu: undefined,\n",
      "      mirostat: undefined,\n",
      "      mirostatEta: undefined,\n",
      "      mirostatTau: undefined,\n",
      "      numBatch: undefined,\n",
      "      numCtx: undefined,\n",
      "      numGpu: undefined,\n",
      "      numKeep: undefined,\n",
      "      numPredict: undefined,\n",
      "      numThread: undefined,\n",
      "      penalizeNewline: undefined,\n",
      "      presencePenalty: undefined,\n",
      "      repeatLastN: undefined,\n",
      "      repeatPenalty: undefined,\n",
      "      temperature: undefined,\n",
      "      stop: undefined,\n",
      "      tfsZ: undefined,\n",
      "      topK: undefined,\n",
      "      topP: undefined,\n",
      "      typicalP: undefined,\n",
      "      useMLock: undefined,\n",
      "      useMMap: undefined,\n",
      "      vocabOnly: undefined,\n",
      "      format: undefined,\n",
      "      client: Ollama {\n",
      "        config: { host: \"http://localhost:11434\" },\n",
      "        fetch: [Function: fetch],\n",
      "        ongoingStreamedRequests: []\n",
      "      }\n",
      "    },\n",
      "    name: undefined\n",
      "  },\n",
      "  lc_runnable: true,\n",
      "  name: undefined,\n",
      "  first: RunnableMap {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: {\n",
      "      steps: {\n",
      "        context: RunnableSequence {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_runnable: true,\n",
      "          name: undefined,\n",
      "          first: [RunnableLambda],\n",
      "          middle: [Array],\n",
      "          last: [RunnableLambda],\n",
      "          lc_namespace: [Array]\n",
      "        },\n",
      "        question: RunnableLambda {\n",
      "          lc_serializable: false,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_runnable: true,\n",
      "          name: undefined,\n",
      "          lc_namespace: [Array],\n",
      "          func: [Function: question]\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    lc_runnable: true,\n",
      "    name: undefined,\n",
      "    lc_namespace: [ \"langchain_core\", \"runnables\" ],\n",
      "    steps: {\n",
      "      context: RunnableSequence {\n",
      "        lc_serializable: true,\n",
      "        lc_kwargs: {\n",
      "          first: [RunnableLambda],\n",
      "          middle: [Array],\n",
      "          last: [RunnableLambda],\n",
      "          name: undefined\n",
      "        },\n",
      "        lc_runnable: true,\n",
      "        name: undefined,\n",
      "        first: RunnableLambda {\n",
      "          lc_serializable: false,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_runnable: true,\n",
      "          name: undefined,\n",
      "          lc_namespace: [Array],\n",
      "          func: [Function (anonymous)]\n",
      "        },\n",
      "        middle: [ [VectorStoreRetriever] ],\n",
      "        last: RunnableLambda {\n",
      "          lc_serializable: false,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_runnable: true,\n",
      "          name: undefined,\n",
      "          lc_namespace: [Array],\n",
      "          func: [Function: convertDocsToString]\n",
      "        },\n",
      "        lc_namespace: [ \"langchain_core\", \"runnables\" ]\n",
      "      },\n",
      "      question: RunnableLambda {\n",
      "        lc_serializable: false,\n",
      "        lc_kwargs: { func: [Function: question] },\n",
      "        lc_runnable: true,\n",
      "        name: undefined,\n",
      "        lc_namespace: [ \"langchain_core\", \"runnables\" ],\n",
      "        func: [Function: question]\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  middle: [\n",
      "    ChatPromptTemplate {\n",
      "      lc_serializable: true,\n",
      "      lc_kwargs: {\n",
      "        inputVariables: [ \"context\", \"question\" ],\n",
      "        promptMessages: [ [HumanMessagePromptTemplate] ],\n",
      "        partialVariables: [Object: null prototype] {}\n",
      "      },\n",
      "      lc_runnable: true,\n",
      "      name: undefined,\n",
      "      lc_namespace: [ \"langchain_core\", \"prompts\", \"chat\" ],\n",
      "      inputVariables: [ \"context\", \"question\" ],\n",
      "      outputParser: undefined,\n",
      "      partialVariables: [Object: null prototype] {},\n",
      "      promptMessages: [\n",
      "        HumanMessagePromptTemplate {\n",
      "          lc_serializable: true,\n",
      "          lc_kwargs: [Object],\n",
      "          lc_runnable: true,\n",
      "          name: undefined,\n",
      "          lc_namespace: [Array],\n",
      "          inputVariables: [Array],\n",
      "          additionalOptions: {},\n",
      "          prompt: [PromptTemplate],\n",
      "          messageClass: undefined,\n",
      "          chatMessageClass: undefined\n",
      "        }\n",
      "      ],\n",
      "      validateTemplate: true\n",
      "    }\n",
      "  ],\n",
      "  last: Ollama {\n",
      "    lc_serializable: true,\n",
      "    lc_kwargs: {\n",
      "      callbacks: undefined,\n",
      "      baseUrl: \"http://localhost:11434\",\n",
      "      model: \"llama3.1\"\n",
      "    },\n",
      "    lc_runnable: true,\n",
      "    name: undefined,\n",
      "    verbose: false,\n",
      "    callbacks: undefined,\n",
      "    tags: [],\n",
      "    metadata: {},\n",
      "    caller: AsyncCaller {\n",
      "      maxConcurrency: Infinity,\n",
      "      maxRetries: 6,\n",
      "      onFailedAttempt: [Function: defaultFailedAttemptHandler],\n",
      "      queue: PQueue {\n",
      "        _events: Events <[Object: null prototype] {}> {},\n",
      "        _eventsCount: 0,\n",
      "        _intervalCount: 0,\n",
      "        _intervalEnd: 0,\n",
      "        _pendingCount: 0,\n",
      "        _resolveEmpty: [Function: empty],\n",
      "        _resolveIdle: [Function: empty],\n",
      "        _carryoverConcurrencyCount: false,\n",
      "        _isIntervalIgnored: true,\n",
      "        _intervalCap: Infinity,\n",
      "        _interval: 0,\n",
      "        _queue: PriorityQueue { _queue: [] },\n",
      "        _queueClass: [class PriorityQueue],\n",
      "        _concurrency: Infinity,\n",
      "        _intervalId: undefined,\n",
      "        _timeout: undefined,\n",
      "        _throwOnTimeout: false,\n",
      "        _isPaused: false\n",
      "      }\n",
      "    },\n",
      "    cache: undefined,\n",
      "    _encoding: undefined,\n",
      "    lc_namespace: [ \"langchain\", \"llms\", \"ollama\" ],\n",
      "    model: \"llama3.1\",\n",
      "    baseUrl: \"http://localhost:11434\",\n",
      "    keepAlive: \"5m\",\n",
      "    embeddingOnly: undefined,\n",
      "    f16KV: undefined,\n",
      "    frequencyPenalty: undefined,\n",
      "    logitsAll: undefined,\n",
      "    lowVram: undefined,\n",
      "    mainGpu: undefined,\n",
      "    mirostat: undefined,\n",
      "    mirostatEta: undefined,\n",
      "    mirostatTau: undefined,\n",
      "    numBatch: undefined,\n",
      "    numCtx: undefined,\n",
      "    numGpu: undefined,\n",
      "    numKeep: undefined,\n",
      "    numPredict: undefined,\n",
      "    numThread: undefined,\n",
      "    penalizeNewline: undefined,\n",
      "    presencePenalty: undefined,\n",
      "    repeatLastN: undefined,\n",
      "    repeatPenalty: undefined,\n",
      "    temperature: undefined,\n",
      "    stop: undefined,\n",
      "    tfsZ: undefined,\n",
      "    topK: undefined,\n",
      "    topP: undefined,\n",
      "    typicalP: undefined,\n",
      "    useMLock: undefined,\n",
      "    useMMap: undefined,\n",
      "    vocabOnly: undefined,\n",
      "    format: undefined,\n",
      "    client: Ollama {\n",
      "      config: { host: \"http://localhost:11434\" },\n",
      "      fetch: [Function: fetch],\n",
      "      ongoingStreamedRequests: []\n",
      "    }\n",
      "  },\n",
      "  lc_namespace: [ \"langchain_core\", \"runnables\" ]\n",
      "}\n",
      "根据原文内容，我可以回答：\n",
      "\n",
      "原文：大厂的架构师，尤其是基础设施团队的人员…\n",
      "\n",
      "答案：基础设施\n"
     ]
    }
   ],
   "source": [
    "import { TextLoader } from 'langchain/document_loaders/fs/text';\n",
    "import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';\n",
    "\n",
    "const loader = new TextLoader('data/qiu.txt');\n",
    "const docs = await loader.load();\n",
    "\n",
    "const splitter = new RecursiveCharacterTextSplitter({\n",
    "  chunkSize: 100,\n",
    "  chunkOverlap: 20,\n",
    "});\n",
    "\n",
    "const splitDocs = await splitter.splitDocuments(docs);\n",
    "\n",
    "import { OllamaEmbeddings } from '@langchain/community/embeddings/ollama';\n",
    "const embeddings = new OllamaEmbeddings({\n",
    "  model: 'llama3.1', // default value\n",
    "  baseUrl: 'http://localhost:11434', // default value\n",
    "  requestOptions: {\n",
    "    useMMap: true, // use_mmap 1\n",
    "    numThread: 10, // num_thread 10\n",
    "    numGpu: 1, // num_gpu 1\n",
    "  },\n",
    "});\n",
    "\n",
    "\n",
    "import { Ollama } from \"@langchain/ollama\";\n",
    "const chatModel = new Ollama({\n",
    "  baseUrl: \"http://localhost:11434\",\n",
    "  model: \"llama3.1\",\n",
    "});\n",
    "\n",
    "\n",
    "import { MemoryVectorStore } from 'langchain/vectorstores/memory';\n",
    "\n",
    "const vectorstore = new MemoryVectorStore(embeddings);\n",
    "await vectorstore.addDocuments(splitDocs);\n",
    "\n",
    "const retriever = vectorstore.asRetriever(2);\n",
    "\n",
    "// const result = await retriever.invoke(\"什么是基础设施\")\n",
    "// console.log('result :', result);\n",
    "\n",
    "import { ChatPromptTemplate } from '@langchain/core/prompts';\n",
    "\n",
    "const TEMPLATE = `\n",
    "你是一个本次事故的分析者，根据文章详细解释和回答问题，你在回答时会引用作品原文。\n",
    "并且回答时仅根据原文，尽可能回答用户问题，如果原文中没有相关内容，你可以回答“原文中没有相关内容”，\n",
    "\n",
    "以下是原文中跟用户回答相关的内容：\n",
    "{context}\n",
    "\n",
    "现在，你需要基于原文，回答以下问题：\n",
    "{question}`;\n",
    "\n",
    "const prompt = ChatPromptTemplate.fromTemplate(TEMPLATE);\n",
    "\n",
    "import { RunnableSequence } from '@langchain/core/runnables';\n",
    "import { Document } from '@langchain/core/documents';\n",
    "\n",
    "const convertDocsToString = (documents: Document[]): string => {\n",
    "  return documents.map((document) => document.pageContent).join('\\n');\n",
    "};\n",
    "\n",
    "const contextRetriverChain = RunnableSequence.from([\n",
    "  (input) => input.question,\n",
    "  retriever,\n",
    "  convertDocsToString,\n",
    "]);\n",
    "\n",
    "const ragChain = RunnableSequence.from([\n",
    "  {\n",
    "    context: contextRetriverChain,\n",
    "    question: (input) => input.question,\n",
    "  },\n",
    "  prompt,\n",
    "  chatModel,\n",
    "]);\n",
    "\n",
    "console.log('开始调用', ragChain);\n",
    "\n",
    "const answer = await ragChain.invoke({\n",
    "  question: '什么是基础设施',\n",
    "});\n",
    "\n",
    "console.log(answer);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deno",
   "language": "typescript",
   "name": "deno"
  },
  "language_info": {
   "name": "typescript"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
